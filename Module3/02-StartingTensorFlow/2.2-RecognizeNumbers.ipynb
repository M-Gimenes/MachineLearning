{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "y_train\n",
    "\n",
    "y_train_converted = keras.utils.to_categorical(y_train)\n",
    "y_test_converted = keras.utils.to_categorical(y_test)\n",
    "\n",
    "x_train_remolded = x_train.reshape((60000, 784))\n",
    "x_test_remolded = x_test.reshape((10000, 784))\n",
    "\n",
    "x_train_normalized = x_train_remolded.astype(\"float32\") / 255\n",
    "x_test_normalized = x_test_remolded.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 200\n",
    "dropout_rate = 0.3\n",
    "l2_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = x_train_normalized.shape[1]\n",
    "n_layer_1 = 30\n",
    "n_layer_2 = 30\n",
    "n_class = y_train_converted.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random.normal([n_input, n_layer_1], stddev=0.05))\n",
    "b1 = tf.Variable(tf.zeros([n_layer_1]))\n",
    "\n",
    "w2 = tf.Variable(tf.random.normal([n_layer_1, n_layer_2], stddev=0.05))\n",
    "b2 = tf.Variable(tf.zeros([n_layer_2]))\n",
    "\n",
    "w_out = tf.Variable(tf.random.normal([n_layer_2, n_class], stddev=0.05))\n",
    "b_out = tf.Variable(tf.zeros([n_class]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2307300567626953, Accuracy: 0.8936166763305664\n",
      "Epoch 2, Loss: 0.17667171359062195, Accuracy: 0.9140333533287048\n",
      "Epoch 3, Loss: 0.15888555347919464, Accuracy: 0.9222999811172485\n",
      "Epoch 4, Loss: 0.14964668452739716, Accuracy: 0.9299166798591614\n",
      "Epoch 5, Loss: 0.14561161398887634, Accuracy: 0.9363833069801331\n",
      "Epoch 6, Loss: 0.14281314611434937, Accuracy: 0.9412500262260437\n",
      "Epoch 7, Loss: 0.13974504172801971, Accuracy: 0.9451500177383423\n",
      "Epoch 8, Loss: 0.13121157884597778, Accuracy: 0.9502666592597961\n",
      "Epoch 9, Loss: 0.12687045335769653, Accuracy: 0.9538666605949402\n",
      "Epoch 10, Loss: 0.12178163230419159, Accuracy: 0.9567999839782715\n"
     ]
    }
   ],
   "source": [
    "def feed_forward(x, training):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, w1), b1))\n",
    "    #layer_1 = tf.nn.dropout(layer_1, rate=dropout_rate if training else 0.0) dropout\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, w2), b2))\n",
    "    #layer_2 = tf.nn.dropout(layer_2, rate=dropout_rate if training else 0.0) dropout\n",
    "    output = tf.matmul(layer_2, w_out) + b_out\n",
    "    return output\n",
    "\n",
    "# Função de perda\n",
    "def loss_fn(logits, labels):\n",
    "    cross = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    #l2_loss =  l2_alpha*(tf.nn.l2_loss(w1)+tf.nn.l2_loss(w2)+tf.nn.l2_loss(w_out)) #regularizador l2\n",
    "    return cross #+ l2_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(0, len(x_train_normalized), batch_size):\n",
    "        x_batch = x_train_normalized[batch:batch + batch_size]\n",
    "        y_batch = y_train_converted[batch:batch + batch_size]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = feed_forward(x_batch, training=True)\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "        \n",
    "        # Calcula e aplica os gradientes\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w_out, b_out])\n",
    "        optimizer.apply_gradients(zip(grads, [w1, b1, w2, b2, w_out, b_out]))\n",
    "\n",
    "    # Cálculo da acurácia (fora do tape)\n",
    "    logits = feed_forward(x_train_normalized, training=False)\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_train_converted, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.numpy()}, Accuracy: {accuracy.numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
