{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "y_train\n",
    "\n",
    "y_train_converted = keras.utils.to_categorical(y_train)\n",
    "y_test_converted = keras.utils.to_categorical(y_test)\n",
    "\n",
    "x_train_remolded = x_train.reshape((60000, 784))\n",
    "x_test_remolded = x_test.reshape((10000, 784))\n",
    "\n",
    "x_train_normalized = x_train_remolded.astype(\"float32\") / 255\n",
    "x_test_normalized = x_test_remolded.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 200\n",
    "dropout_rate = 0.3\n",
    "l2_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = x_train_normalized.shape[1]\n",
    "n_layer_1 = 30\n",
    "n_layer_2 = 30\n",
    "n_class = y_train_converted.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random.normal([n_input, n_layer_1], stddev=0.05))\n",
    "b1 = tf.Variable(tf.zeros([n_layer_1]))\n",
    "\n",
    "w2 = tf.Variable(tf.random.normal([n_layer_1, n_layer_2], stddev=0.05))\n",
    "b2 = tf.Variable(tf.zeros([n_layer_2]))\n",
    "\n",
    "w_out = tf.Variable(tf.random.normal([n_layer_2, n_class], stddev=0.05))\n",
    "b_out = tf.Variable(tf.zeros([n_class]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2276322990655899, Train Accuracy: 0.8881833553314209, Test Accuracy: 0.8880000114440918\n",
      "Epoch 2, Loss: 0.1814473271369934, Train Accuracy: 0.9131666421890259, Test Accuracy: 0.9125999808311462\n",
      "Epoch 3, Loss: 0.1640845239162445, Train Accuracy: 0.9229833483695984, Test Accuracy: 0.9222999811172485\n",
      "Epoch 4, Loss: 0.15051594376564026, Train Accuracy: 0.9315999746322632, Test Accuracy: 0.9289000034332275\n",
      "Epoch 5, Loss: 0.1363469362258911, Train Accuracy: 0.9394166469573975, Test Accuracy: 0.9358000159263611\n",
      "Epoch 6, Loss: 0.12448141723871231, Train Accuracy: 0.9456999897956848, Test Accuracy: 0.9417999982833862\n",
      "Epoch 7, Loss: 0.1163296326994896, Train Accuracy: 0.9503833055496216, Test Accuracy: 0.9476000070571899\n",
      "Epoch 8, Loss: 0.11060789227485657, Train Accuracy: 0.9544833302497864, Test Accuracy: 0.9495000243186951\n",
      "Epoch 9, Loss: 0.10618840903043747, Train Accuracy: 0.9571833610534668, Test Accuracy: 0.9521999955177307\n",
      "Epoch 10, Loss: 0.1048191636800766, Train Accuracy: 0.9596666693687439, Test Accuracy: 0.9539999961853027\n"
     ]
    }
   ],
   "source": [
    "def feed_forward(x, training):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, w1), b1))\n",
    "    #layer_1 = tf.nn.dropout(layer_1, rate=dropout_rate if training else 0.0) dropout\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, w2), b2))\n",
    "    #layer_2 = tf.nn.dropout(layer_2, rate=dropout_rate if training else 0.0) dropout\n",
    "    output = tf.matmul(layer_2, w_out) + b_out\n",
    "    return output\n",
    "\n",
    "# Função de perda\n",
    "def loss_fn(logits, labels):\n",
    "    cross = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    #l2_loss =  l2_alpha*(tf.nn.l2_loss(w1)+tf.nn.l2_loss(w2)+tf.nn.l2_loss(w_out)) #regularizador l2\n",
    "    return cross #+ l2_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(0, len(x_train_normalized), batch_size):\n",
    "        x_batch = x_train_normalized[batch:batch + batch_size]\n",
    "        y_batch = y_train_converted[batch:batch + batch_size]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = feed_forward(x_batch, training=True)\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "        \n",
    "        # Calcula e aplica os gradientes\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w_out, b_out])\n",
    "        optimizer.apply_gradients(zip(grads, [w1, b1, w2, b2, w_out, b_out]))\n",
    "\n",
    "    # Cálculo da acurácia (fora do tape)\n",
    "    logits_train = feed_forward(x_train_normalized, training=False)  # `training=False` desativa Dropout\n",
    "    correct_prediction_train = tf.equal(tf.argmax(logits_train, 1), tf.argmax(y_train_converted, 1))\n",
    "    accuracy_train = tf.reduce_mean(tf.cast(correct_prediction_train, tf.float32))\n",
    "\n",
    "    logits_test = feed_forward(x_test_normalized, training=False)  # `training=False` desativa Dropout\n",
    "    correct_prediction_test = tf.equal(tf.argmax(logits_test, 1), tf.argmax(y_test_converted, 1))\n",
    "    accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32))\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.numpy()}, Train Accuracy: {accuracy_train.numpy()}, Test Accuracy: {accuracy_test.numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
